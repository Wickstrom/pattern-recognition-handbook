{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a88fdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data transformation and dimensionality reduction (DTDR) I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1062d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- So far, the course has been problem oriented (mainly classification).\n",
    "- However, an essential part of pattern recognition is data analysis.\n",
    "- In this lecture, we will look at methods that compress data into a more compact representation thorugh linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c54d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analyzing data\n",
    "\n",
    "- Show example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b9d46",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problems\n",
    "\n",
    "- The data dimensionality may be very large.\n",
    "    - Computationally demanding.\n",
    "    - Curse of dimensionality\n",
    "- Some parts of the data may not be discriminiative / be redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46a61f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Remedies\n",
    "\n",
    "- May pick only parts of the data to use.\n",
    "- May transform the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d246d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Approaches to DTDR\n",
    "\n",
    "- To better discriminate between classes (supervised).\n",
    "- To remove redundancy (unsupervised)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b159af5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fisher discriminant analysis (FDA)\n",
    "\n",
    "- Transform (project) to 1D:\n",
    "- Start with the two class case and $P(w_1)=P(w_2)$\n",
    "- Fisher discriminant ratio:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8fb096",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scatter matrices\n",
    "\n",
    "- **Within class:** $$ \\boldsymbol{S}_w = \\sum_{i=1}^M P(\\boldsymbol{w}_i) \\boldsymbol{\\Sigma}_i $$\n",
    "\n",
    "- where $$\\boldsymbol{\\Sigma}_i = \\mathbb{E}[(\\boldsymbol{x} - \\boldsymbol{\\mu}_i)(\\boldsymbol{x} - \\boldsymbol{\\mu}_i)^T]$$\n",
    "\n",
    "- Example:\n",
    "\n",
    "---\n",
    "\n",
    "- **Between class:** $$ \\boldsymbol{S}_B = \\sum_{i=1}^M P(\\boldsymbol{w}_i) (\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu})^2 $$\n",
    "\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009b91a",
   "metadata": {},
   "source": [
    "### Remark\n",
    "\n",
    "- Class separability measures in $\\mathbf{x}$ by e.g. $$\\frac{\\text{trace}(\\boldsymbol{S}_w)}{\\text{trace}(\\boldsymbol{S}_B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcbe5ba",
   "metadata": {},
   "source": [
    "### Fisher discriminant analysis\n",
    "\n",
    "- Remember; want to learn a transformation into 1D $$z = \\mathbf{w}^T \\mathbf{x} \\Rightarrow \\mu = \\mathbf{w}^T \\boldsymbol{\\mu}$$\n",
    "\n",
    "- $$S_B = (\\mu_1 - \\mu_2)^2 = $$\n",
    "\n",
    "- $$ \\sigma^2 = \\mathbb{E}[(x - \\mu)^2] = $$\n",
    "\n",
    "- $$ S_w = \\sigma_1^2 + \\sigma_2^2 = $$\n",
    "\n",
    "- Hence: Fisher discriminant ratio (FDR):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0d2c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Remarks\n",
    "\n",
    "- Don't need $P(w_1) = P(w_2)$, but easy to solve.\n",
    "\n",
    "- If $P(w_1) \\neq P(w_2)$: $\\mathbf{w}$ is the leading eigenvector of $\\boldsymbol{S}_w^{-1} \\boldsymbol{S}_B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214f5e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### FDR\n",
    "\n",
    "- Want: $$\\arg\\max_{\\mathbf{w}} \\frac{\\mathbf{w}^T \\boldsymbol{S}_B \\mathbf{w}}{\\mathbf{w}^T \\boldsymbol{S}_w \\mathbf{w}}$$\n",
    "\n",
    "- At solution (problem x.x): $$\\boldsymbol{S}_w \\mathbf{w} = \\lambda \\boldsymbol{S}_B \\mathbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9dc7c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##\n",
    "\n",
    "I. If $P(w_1) = P(w_2)$:\n",
    "$$\n",
    "\\lambda \\boldsymbol{S}_w \\mathbf{w} = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\mathbf{w}\n",
    "$$\n",
    "$\\implies$ $\\mathbf{w} \\propto \\boldsymbol{S}_w^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)$\n",
    "\n",
    "---\n",
    "\n",
    "II. If $P(w_1) \\neq P(w_2)$:\n",
    "$$\n",
    "\\mathbf{w} = \\text{leading eigenvector of } \\boldsymbol{S}_w^{-1} \\boldsymbol{S}_B\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**  \n",
    "$P(w_1) = P(w_2)$\n",
    "\n",
    "$$\n",
    "S_w = \\frac{1}{2} \\sigma_1^2 + \\frac{1}{2} \\sigma_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_B = \\frac{1}{2} (\\mu_1 - \\mu)^2 + \\frac{1}{2} (\\mu_2 - \\mu)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c86b17",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Remark\n",
    "\n",
    "- Generalized to $z = \\mathbf{w}^T \\mathbf{x} \\in \\mathbb{R}^k$ where $k \\leq d$.\n",
    "    - More complex (pages 291-297 in book)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265d167",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "- First: $\\mathbf{z} = \\mathbf{A} \\mathbf{x}$ such that $\\mathbf{z} \\in \\mathbb{R}^d$, $\\mathbf{x} \\in \\mathbb{R}^d$, and $\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\n",
    "$\n",
    "- Want: \\boldsymbol{\\Sigma}_y diagonal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2392aa5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A closer look at the covariance matrix\n",
    "\n",
    "- Have:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_y = \\mathbb{E}[(\\mathbf{y} - \\boldsymbol{\\mu}_y)(\\mathbf{y} - \\boldsymbol{\\mu}_y)^T]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\mathbb{E}[(\\mathbf{A}\\mathbf{x} - \\mathbf{A}\\boldsymbol{\\mu}_x)(\\mathbf{A}\\mathbf{x} - \\mathbf{A}\\boldsymbol{\\mu}_x)^T]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "- $\\boldsymbol{\\Sigma}_x$: symmetric and positive semi-definite $\\implies$ orthogonal eigenvectors and non-negative eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efed7e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Eigendecomposition of the covariance matrix\n",
    "\n",
    "- Let $\\mathbf{E} = [\\mathbf{e}_1, \\ldots, \\mathbf{e}_d]$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_x \\mathbf{E} = \\mathbf{E} \\boldsymbol{\\Lambda}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\boldsymbol{\\Lambda} = \\begin{bmatrix}\n",
    "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\lambda_d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "(diagonal matrix of eigenvalues)\n",
    "\n",
    "---\n",
    "\n",
    "- Have:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65026829",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Interpreting the eigenvalues and eigenvectors\n",
    "\n",
    "- Note: $$\\boldsymbol{\\Sigma}_y=$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732740cf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variance maximally preserved\n",
    "\n",
    "- First: $$ \\sum_{i=1}^d \\text{Var}(y_i) = \\sum_{i=1}^d \\lambda_i $$\n",
    "\n",
    "- Thus: Let $\\mathbf{A} = \\mathbf{E} = [\\mathbf{e}_1, \\ldots, \\mathbf{e}_d]$\n",
    "\n",
    "- Remark: Let $\\mathbb{E}[\\mathbf{x}] = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81655ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA is reconstruction / compression\n",
    "\n",
    "- Let $\\mathbf{z} \\in \\mathbb{R}^d = [z(0), z(1), \\ldots, z(d-1)]^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c30f3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA is reconstruction / compression\n",
    "\n",
    "- Have $$\\mathbf{x} = \\mathbf{A} \\mathbf{z}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca8c63",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA is reconstruction / compression\n",
    "\n",
    "- **MSE:** $$ \\mathbb{E}\\left[\\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|^2\\right] $$\n",
    "\n",
    "For $\\mathbf{z} \\in \\mathbb{R}^k$: $$ \\hat{\\mathbf{x}} = \\sum_{i=0}^{k-1} z(i) \\mathbf {e}_i $$\n",
    "\n",
    "If $y_i = 0$ for $i \\geq k$:\n",
    "$$\n",
    "\\mathbb{E}\\left[\\|\\mathbf{x} - \\hat{\\mathbf{x}}\\|^2\\right] = \\sum_{i=k}^{d-1} \\lambda_i\n",
    "$$\n",
    "\n",
    "$\\implies$ **MSE is minimized**.\n",
    "\n",
    "---\n",
    "\n",
    "- **Compression:** Store/save $\\mathbf{z} \\in \\mathbb{R}^k$ instead of $\\mathbf{x}$ (e.g. images).\n",
    "- **Reconstruct:** $\\hat{\\mathbf{x}}$ using $\\mathbf{z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebd1459",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programming exercises\n",
    "\n",
    "Below are programming exercises assocaited with this lecture. These cell blocks are starting points that loads the data and prepares the problem such that you can get going with the implementation. There are also theoretical exercsies, but due to copyright we cannot shared them here. They will be made available in a private repository connected to the course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109b068",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality reduction on the Iris dataset\n",
    "\n",
    "We will now revisit the Iris dataset from the first lectures. Instead of simply selecting 2 features, you will now perform either FDA or PCA to reduce the dimensionality down to 2 and visualize the data. Do you observe any difference between the two methods?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b291ced",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "iris = fetch_ucirepo(id=53)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = iris.data.features\n",
    "feature_1_name = 'sepal length (cm)'\n",
    "feature_2_name = 'sepal width (cm)'\n",
    "y = np.zeros(150)\n",
    "y[50:100] = 1\n",
    "y[100:150] = 2\n",
    "y_names = np.unique(iris.data.targets)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
