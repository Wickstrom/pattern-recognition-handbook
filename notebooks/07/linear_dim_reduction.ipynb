{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a88fdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data transformation and dimensionality reduction (DTDR) I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1062d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- So far, the course has been problem oriented (mainly classification).\n",
    "- However, an essential part of pattern recognition is data analysis.\n",
    "- In this lecture, we will look at methods that compress data into a more compact representation thorugh linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c54d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analyising data\n",
    "\n",
    "- How do\n",
    "- adf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b9d46",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problems\n",
    "\n",
    "- The data dimensionality may be very large.\n",
    "    - Computationally demanding.\n",
    "    - Curse of dimensionality\n",
    "- Some parts of the data may not be discriminiative / be redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46a61f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Remedies\n",
    "\n",
    "- May pick only parts of the data to use.\n",
    "- May transform the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d246d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Approaches to DTDR\n",
    "\n",
    "- To better discriminate between classes (supervised).\n",
    "- To remove redundancy (unsupervised)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b159af5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fisher discriminant analysis (FDA)\n",
    "\n",
    "- Transform (project) to 1D:\n",
    "- Start with the two class case and $P(w_1)=P(w_2)$\n",
    "- Fisher discriminant ratio:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8fb096",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scatter matrices\n",
    "\n",
    "- **Within class:** $$ \\boldsymbol{S}_w = \\sum_{i=1}^M P(\\boldsymbol{w}_i) \\boldsymbol{\\Sigma}_i $$\n",
    "\n",
    "- where $$\\boldsymbol{\\Sigma}_i = \\mathbb{E}[(\\boldsymbol{x} - \\boldsymbol{\\mu}_i)(\\boldsymbol{x} - \\boldsymbol{\\mu}_i)^T]$$\n",
    "\n",
    "- Example:\n",
    "\n",
    "---\n",
    "\n",
    "- **Between class:** $$ \\boldsymbol{S}_B = \\sum_{i=1}^M P(\\boldsymbol{w}_i) (\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu})^2 $$\n",
    "\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009b91a",
   "metadata": {},
   "source": [
    "### Remark\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0d2c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Remarks\n",
    "\n",
    "- Don't need $P(w_1) = P(w_2)$, but easy to solve.\n",
    "\n",
    "- If $P(w_1) \\neq P(w_2)$: $\\mathbf{w}$ is the leading eigenvector of $\\boldsymbol{S}_w^{-1} \\boldsymbol{S}_B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214f5e9",
   "metadata": {},
   "source": [
    "### balbalba\n",
    "\n",
    "- Want: $$\\arg\\max_{\\mathbf{w}} \\frac{\\mathbf{w}^T \\boldsymbol{S}_B \\mathbf{w}}{\\mathbf{w}^T \\boldsymbol{S}_w \\mathbf{w}}$$\n",
    "\n",
    "- At solution (problem x.x): $$\\boldsymbol{S}_w \\mathbf{w} = \\lambda \\boldsymbol{S}_B \\mathbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9dc7c",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "I. If $P(w_1) = P(w_2)$:\n",
    "$$\n",
    "\\lambda \\boldsymbol{S}_w \\mathbf{w} = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\mathbf{w}\n",
    "$$\n",
    "$\\implies$ $\\mathbf{w} \\propto \\boldsymbol{S}_w^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)$\n",
    "\n",
    "---\n",
    "\n",
    "II. If $P(w_1) \\neq P(w_2)$:\n",
    "$$\n",
    "\\mathbf{w} = \\text{leading eigenvector of } \\boldsymbol{S}_w^{-1} \\boldsymbol{S}_B\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**  \n",
    "$P(w_1) = P(w_2)$\n",
    "\n",
    "$$\n",
    "S_w = \\frac{1}{2} \\sigma_1^2 + \\frac{1}{2} \\sigma_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_B = \\frac{1}{2} (\\mu_1 - \\mu)^2 + \\frac{1}{2} (\\mu_2 - \\mu)^2\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
