{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- Ranking is a fundamental task in data analysis.\n",
    "- Not only content, but link between content!\n",
    "- Most famous example, Google web-page ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "- Consider the following directed graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Idea 1\n",
    "\n",
    "- Count backlinks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Idea 2\n",
    "\n",
    "- Count ranks associated with backlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Remarks to idea 2\n",
    "\n",
    "- Ranking becomes maths!\n",
    "- Unlogical \"recommendation\" process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Idea 3\n",
    "\n",
    "- Multiple recommendations sum to one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Idea 3 in matrix-vector form\n",
    "\n",
    "- Let $$ \\mathbf{H}^T = \\begin{bmatrix} 0 & 0 & 1 & \\frac{1}{2} \\\\ \\frac{1}{3} & 0 & 0 \\\\ \\frac{1}{3} & \\frac{1}{2} & 0 & \\frac{1}{2} \\\\ \\frac{1}{3} & \\frac{1}{2} & 0 & 0 \\end{bmatrix}$$\n",
    "- and $$ \\boldsymbol{\\Pi} = \\begin{bmatrix} r_1 \\\\ r_2 \\\\ r_3 \\\\ r_4 \\end{bmatrix}$$\n",
    "- Hence: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Note on idea 3\n",
    "\n",
    "- $\\boldsymbol{\\Pi}$ is an eigenvector of $\\mathbf{H}^T$ for $\\lambda = 1$\n",
    "- World's most famous eigenvector\n",
    "- Google PageRank alogirthm (with some modifications)\n",
    "- Solution: $$ \\boldsymbol{\\Pi} = \\begin{bmatrix} 0.4 \\\\ 0.1 \\\\ 0.3 \\\\ 0.2 \\end{bmatrix}$$\n",
    "- Alternatively: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Google as a Markov chain\n",
    "\n",
    "- $\\mathbf{H}$ is special!\n",
    "    - Transition matrix in (homogeneous, discrete) Markov chain\n",
    "- $$ \\mathbf{H} = \\begin{bmatrix} 0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\\\ 1 & 0 & 0 & 0 \\\\ \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Google as a Markov chain\n",
    "\n",
    "- $\\boldsymbol{\\Pi}$ has a special meaning\n",
    "- Theorem: If $\\mathbf{H}: H_{ij}=p_{ij}$ is a transition matrix and $0 \\lt p_{ij} \\lt 1$ then\n",
    "    - $\\lambda_{max}=1$\n",
    "    - $\\boldsymbol{\\Pi}_{max}$ is unique, positive, sums to one.\n",
    "    - $\\boldsymbol{\\Pi}_{max}$ can always be found by the power method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Markov chain example\n",
    "\n",
    "- $$ \\mathbf{A}(\\mathbf{H}) = \\begin{bmatrix} 0.3 & 0.4 & 0.5 \\\\ 0.3 & 0.4 & 0.3 \\\\ 0.4 & 0.2 & 0.2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Markov chain example\n",
    "\n",
    "- Find:\n",
    "    - P(start @ C, end @ B after 2 deliveries)\n",
    "- $P(CA)P(AB)+P(CB)P(BB)+P(CC)P(CB)=0.33$\n",
    "- $$ \\begin{bmatrix} 0.3 & 0.4 & 0.5 \\\\ 0.3 & 0.4 & 0.3 \\\\ 0.4 & 0.2 & 0.2 \\end{bmatrix} = \\mathbf{A}\\mathbf{A}=\\mathbf{A}^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov chain example\n",
    "\n",
    "- Probablities after 5 and 6 steps:\n",
    "- $$ \\begin{bmatrix} 0.3 & 0.4 & 0.5 \\\\ 0.3 & 0.4 & 0.3 \\\\ 0.4 & 0.2 & 0.2 \\end{bmatrix} = \\mathbf{A}\\mathbf{A}=\\mathbf{A}^2$$\n",
    "- Converges!\n",
    "- Stationary distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov chain example\n",
    "\n",
    "- Eigenvector:\n",
    "    - Let $\\mathbf{x}^{(0)}$\n",
    "- $$ \\begin{bmatrix} 0.3 & 0.4 & 0.5 \\\\ 0.3 & 0.4 & 0.3 \\\\ 0.4 & 0.2 & 0.2 \\end{bmatrix} = \\mathbf{A}\\mathbf{A}=\\mathbf{A}^2$$\n",
    "- Converges!\n",
    "- Stationary distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The power method\n",
    "\n",
    "- Iterative; finds dominant eigenvector\n",
    "- Scaling after iteration in general.\n",
    "- Markov example:\n",
    "    - $\\mathbf{x}^T = [0.39, 0.33, 0.28]$ is the stationary distribution off the states in the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea 4\n",
    "\n",
    "- Modify $\\mathbf{H}$ such that $0 \\lt p_{ij} \\lt 1$.\n",
    "- Modification 1; Dangling nodes (no outlinks) creates zero rows in $\\mathbf{H}$\n",
    "- $$ \\mathbf{H}^T = \\begin{bmatrix} 0 & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\\\ 1 & 0 & 0 & 0 & 0 \\\\ \\frac{1}{2} & 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\end{bmatrix}$$\n",
    "- Replace zero row with row of 1/n probablities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modification 2\n",
    "\n",
    "- Google matrix: $$ \\mathbf{G} = \\alpha \\mathbf{S} + (1-\\alpha)\\mathbf{E}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Interpretation $(\\alpha=0.85)$\n",
    "\n",
    "- 85% of the time, surfer follow links.\n",
    "- 15% of the time, surfer types URL (teleportation).\n",
    "- Personalization:\n",
    "    - $\\frac{1}{n}\\mathbf{e}\\mathbf{e}^T \\rightarrow \\mathbf{e}\\mathbf{v}^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final remarks\n",
    "\n",
    "- $\\mathbf{H}$ is sparse (good).\n",
    "- $\\mathbf{G}$ is dense but function of $\\mathbf{H}$\n",
    "- Power method is nice! (no inverse)\n",
    "- Early report from Google indicated 50 iterations.\n",
    "- Updated on a frequent basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programming exercises\n",
    "\n",
    "Below are programming exercises assocaited with this lecture. These cell blocks are starting points that loads the data and prepares the problem such that you can get going with the implementation. There are also theoretical exercsies, but due to copyright we cannot shared them here. They will be made available in a private repository connected to the course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Making the Two-Moons dataset linearly separable\n",
    "\n",
    "The code below loads a classic synthetic machine learning dataset, the Two Moons dataset, that we have looked at before. Use either kernel PCA or Laplacian Eigenmaps (or both if you wish) to transform the Two-Moons dataset. This problem is more in the data transformation spirit than in the dimensionality reduction, but illustrates the power of the non-linear methods introduced in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pr-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
