{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "- For the final part of non-linear classifiers, we will see how we can turn SVMs into non-linear classifiers.\n",
    "- Heavily depends on a field of research known as **kernel methods**.\n",
    "- A field of its own with lots of use cases throughout machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remembering SVMs\n",
    "\n",
    "- **Recall:**  From $L(\\mathbf{w}, w_0)$: $$\\mathbf{w} = \\sum_{i \\in SV} \\lambda_i y_i \\mathbf{x}_i$$\n",
    "\n",
    "- **Dual:**  $$ \\max_{\\lambda \\geq 0} \\sum_{i=1}^N \\lambda_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j y_i y_j \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle$$\n",
    "\n",
    "- Subject to: $$ \\sum_i \\lambda_i y_i = 0$$\n",
    "- Note: $\\langle \\mathbf{x}_i, \\mathbf{x} \\rangle$ denotes the inner product\n",
    "\n",
    "---\n",
    "\n",
    "- **In testing:** $$ g(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + w_0 = \\sum_{i \\in SV} \\lambda_i y_i \\langle \\mathbf{x}_i, \\mathbf{x} \\rangle + w_0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example with explicit mapping\n",
    "\n",
    "- Explicitly map $\\mathbf{x}$, then use linear SVM.\n",
    "\n",
    "- Let $$ \\mathbf{z} = \\begin{bmatrix} x_1^2 \\\\ \\sqrt{2} x_1 x_2 \\\\ x_2^2 \\end{bmatrix} $$ where $$\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$$\n",
    "\n",
    "---\n",
    "\n",
    "- **Need:** $$\\mathbf{z}_i^T \\mathbf{z}_j = x_{i1}^2 x_{j1}^2 + 2 x_{i1} x_{i2} x_{j1} x {j2} + x_{i2}^2 x_{j2}^2 = $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example with explicit mapping - training and testing\n",
    "\n",
    "- **Training:**: $$ \\sum_{i=1}^N \\lambda_i + \\sum_{i=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j y_i y_j \\mathbf{z}_i^T \\mathbf{z}_j $$\n",
    "\n",
    "- $$ \\sum_{i=1}^N \\lambda_i + \\sum_{i=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j y_i y_j K(\\mathbf{x}_i,\\mathbf{x}_j) $$\n",
    "\n",
    "- **Testing:**  $$ g(\\mathbf{z}) = \\sum_{\\mathbf{z}_i \\in SV} \\lambda_i y_i \\mathbf{z}_i^T \\mathbf{z}_j$$\n",
    " \n",
    "- $$ g(\\mathbf{x}) = \\sum_{\\mathbf{x}_i \\in SV} \\lambda_i y_i K(\\mathbf{x}_i, \\mathbf{x})$$\n",
    "\n",
    "- Can always find inner-product kernel $K(\\mathbf{x}_i, \\mathbf{x}_j)$!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mercer's theorem\n",
    "\n",
    "- $$K(\\mathbf{x}_i,\\mathbf{x}_j) = $$\n",
    "\n",
    "- [Nice open access article on kernel methods for those who want to learn more.](https://arxiv.org/pdf/math/0701907)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernels\n",
    "\n",
    "- **Polynomials:**  $$ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\left(\\mathbf{x}_i^T,\\mathbf{x}_j+1\\right)^q \\text{, where } q>0$$\n",
    "\n",
    "- **RBF:**  $$ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{1}{2\\sigma^2} \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2\\right)$$\n",
    "\n",
    "- **Tanh:**  $$ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh\\left(\\beta\\, \\mathbf{x}_i^T \\mathbf{x}_j + \\gamma\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-linear SVM\n",
    "\n",
    "- Choose $K(\\mathbf{x}_i, \\mathbf{x}_j)$\n",
    "\n",
    "- **Training:**  $$\\max_{\\lambda \\geq 0} \\sum_{i=1}^N \\lambda_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)$$\n",
    "\n",
    "- **Test:**  $$g(\\mathbf{x}) = \\sum_{i \\in SV} \\lambda_i y_i K(\\mathbf{x}_i, \\mathbf{x}) +w_0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Non-linear SVM as a network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Non-separable classes\n",
    "\n",
    "- Remember: $$ \\mathbf{w}^T \\mathbf{x} + w_0  \\geq 1-\\gamma$$\n",
    "- Both classes: $$ y_i \\left(\\mathbf{w}^T \\mathbf{x}_i + w_0\\right)  \\geq 1-\\gamma_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Non-separable classes - in practice\n",
    "\n",
    "- Don't want too many $\\gamma_i > 0$\n",
    "- Minimize: $$J(\\mathbf{w}, w_0, \\gamma_i) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^N \\gamma_i$$\n",
    "- Subject to: $$ y_i \\left( \\mathbf{w}^T \\mathbf{x}_i + w_0 \\right) \\geq 1 - \\gamma_i$$\n",
    "\n",
    "$$\n",
    "\\gamma_i \\geq 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "- **Dual:** $$ \\max_{\\lambda \\geq 0} \\sum_{i=1}^N \\lambda_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\lambda_i \\lambda_j y_i y_j \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle $$\n",
    "\n",
    "- Subject to: $$ \\sum_i \\lambda_i y_i = 0$$\n",
    "\n",
    "$$\n",
    "0 \\leq \\lambda_i \\leq C\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical considerations for non-linear SVMs\n",
    "\n",
    "- Start simple -> linear kernel.\n",
    "    - Only two hyperparameters to consider; slack variable and tolerance for stopping criterion.\n",
    "- Then -> non-linear kernel. An RBF kernel is the standard choice.\n",
    "    - Added complexity; kernel width.\n",
    "- Use validation data to select hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programming exercises\n",
    "\n",
    "Below are programming exercises assocaited with this lecture. These cell blocks are starting points that loads the data and prepares the problem such that you can get going with the implementation. There are also theoretical exercsies, but due to copyright we cannot shared them here. They will be made available in a private repository connected to the course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-linear classification with a SVM\n",
    "\n",
    "The code below loads a classic synthetic machine learning dataset, the Two Moons dataset, that we have looked at before. Traing a SVM with a non-linear kernel to tackle this non-linearly separable classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.15, random_state=42)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wine classification with SVM\n",
    "\n",
    "This problem is revisting the Wine classification problem from the linear SVM notebook. Now, turn the SVM into a non-linear classifier through a Gaussian kernel. How does this affect performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# fetch dataset\n",
    "wine_data = load_wine()\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = wine_data.data[:, :2]\n",
    "feature_1_name = 'alcohol'\n",
    "feature_2_name = 'malic acid'\n",
    "y = wine_data.target\n",
    "y_names = np.unique(y)  # class names\n",
    "colors = ['black', 'blue', 'red']\n",
    "\n",
    "plt.figure(1, figsize=(5, 5))\n",
    "for class_value, color in zip(y_names, colors):\n",
    "    plt.scatter(X[y == class_value, 0], X[y == class_value, 1], s=120, facecolors='none',\n",
    "                edgecolors=color, linewidth=3.0, label=f'Class {class_value+1}')\n",
    "plt.xlabel(feature_1_name)\n",
    "plt.ylabel(feature_2_name)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "sta2003_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "403px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
